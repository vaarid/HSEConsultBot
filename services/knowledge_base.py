"""
–ú–æ–¥—É–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π FAQ –ø–æ –æ—Ö—Ä–∞–Ω–µ —Ç—Ä—É–¥–∞
"""
import json
import aiohttp
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from datetime import datetime
from difflib import SequenceMatcher

from utils.logger import setup_logger

logger = setup_logger()


class KnowledgeBase:
    """
    –ö–ª–∞—Å—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π FAQ
    """
    
    def __init__(self, faq_file_path: str = "faq_ohs_ru_links.json"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        
        Args:
            faq_file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å FAQ
        """
        self.faq_file_path = Path(faq_file_path)
        self.faq_data: List[Dict] = []
        self._load_faq()
    
    def _load_faq(self):
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å FAQ –∏–∑ JSON —Ñ–∞–π–ª–∞
        """
        try:
            if not self.faq_file_path.exists():
                logger.error(f"–§–∞–π–ª FAQ –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.faq_file_path}")
                return
            
            with open(self.faq_file_path, 'r', encoding='utf-8') as f:
                self.faq_data = json.load(f)
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.faq_data)} –≤–æ–ø—Ä–æ—Å–æ–≤ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ FAQ: {e}", exc_info=True)
    
    def reload_faq(self):
        """
        –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç—å FAQ –∏–∑ —Ñ–∞–π–ª–∞
        """
        logger.info("–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π FAQ")
        self._load_faq()
    
    def get_all_questions(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤
        """
        return [item['question'] for item in self.faq_data]
    
    def get_blocks(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±–ª–æ–∫–æ–≤ (–∫–∞—Ç–µ–≥–æ—Ä–∏–π)
        
        Returns:
            List[str]: –°–ø–∏—Å–æ–∫ –±–ª–æ–∫–æ–≤
        """
        blocks = set(item['block'] for item in self.faq_data)
        return sorted(blocks)
    
    def get_questions_by_block(self, block_name: str) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –≤–æ–ø—Ä–æ—Å—ã –ø–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –±–ª–æ–∫—É
        
        Args:
            block_name: –ù–∞–∑–≤–∞–Ω–∏–µ –±–ª–æ–∫–∞
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –±–ª–æ–∫–µ
        """
        return [
            item for item in self.faq_data 
            if item['block'] == block_name
        ]
    
    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """
        –†–∞—Å—Å—á–∏—Ç–∞—Ç—å —Å—Ö–æ–∂–µ—Å—Ç—å –¥–≤—É—Ö —Ç–µ–∫—Å—Ç–æ–≤ (–ø—Ä–æ—Å—Ç–æ–π –º–µ—Ç–æ–¥)
        
        Args:
            text1: –ü–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –í—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç
            
        Returns:
            float: –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)
        """
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text1_lower = text1.lower()
        text2_lower = text2.lower()
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º SequenceMatcher –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        similarity = SequenceMatcher(None, text1_lower, text2_lower).ratio()
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –±–æ–Ω—É—Å –∑–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        words1 = set(text1_lower.split())
        words2 = set(text2_lower.split())
        
        # –£–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        stop_words = {'—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∫—Ç–æ', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', 
                      '–∫–∞–∫–∏–µ', '–Ω—É–∂–Ω–æ', '–º–æ–∂–Ω–æ', '–ª–∏', '–≤', '–Ω–∞', '–ø–æ', '—Å', 
                      '–∏', '–∏–ª–∏', '–∞', '–Ω–æ', '—ç—Ç–æ', '—Ç–æ', '–¥–∞', '–Ω–µ—Ç', '–¥–ª—è',
                      '–ø—Ä–∏', '–æ', '–æ–±', '–æ—Ç', '–¥–æ', '–∏–∑', '—É', '–∫'}
        
        words1_filtered = words1 - stop_words
        words2_filtered = words2 - stop_words
        
        if words1_filtered and words2_filtered:
            # Jaccard similarity –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            intersection = len(words1_filtered & words2_filtered)
            union = len(words1_filtered | words2_filtered)
            keyword_similarity = intersection / union if union > 0 else 0
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ–±–∞ –º–µ—Ç–æ–¥–∞
            similarity = (similarity * 0.6) + (keyword_similarity * 0.4)
        
        return similarity
    
    def find_relevant_questions(
        self, 
        query: str, 
        threshold: float = 0.5, 
        top_k: int = 5
    ) -> List[Tuple[Dict, float]]:
        """
        –ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏ (0-1)
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            List[Tuple[Dict, float]]: –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (–≤–æ–ø—Ä–æ—Å, –æ—Ü–µ–Ω–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏)
        """
        if not self.faq_data:
            logger.warning("–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –ø—É—Å—Ç–∞")
            return []
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
        similarities = []
        
        for item in self.faq_data:
            question = item['question']
            similarity = self._calculate_similarity(query, question)
            
            if similarity >= threshold:
                similarities.append((item, similarity))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        return similarities[:top_k]
    
    async def check_url_validity(self, url: str, timeout: int = 5) -> Tuple[bool, Optional[int]]:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å URL (–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å)
        
        Args:
            url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            timeout: –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            
        Returns:
            Tuple[bool, Optional[int]]: (–¥–æ—Å—Ç—É–ø–µ–Ω, —Å—Ç–∞—Ç—É—Å-–∫–æ–¥)
        """
        if not url or url.strip() == "":
            return False, None
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.head(
                    url, 
                    timeout=aiohttp.ClientTimeout(total=timeout),
                    allow_redirects=True
                ) as response:
                    # –°—á–∏—Ç–∞–µ–º URL –≤–∞–ª–∏–¥–Ω—ã–º, –µ—Å–ª–∏ —Å—Ç–∞—Ç—É—Å 200-399
                    is_valid = 200 <= response.status < 400
                    return is_valid, response.status
                    
        except aiohttp.ClientError as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ URL {url}: {e}")
            return False, None
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ URL {url}: {e}")
            return False, None
    
    async def get_answer_with_validation(
        self, 
        query: str, 
        check_urls: bool = True
    ) -> Optional[Dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π URL
        
        Args:
            query: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            check_urls: –ü—Ä–æ–≤–µ—Ä—è—Ç—å –ª–∏ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å URL
            
        Returns:
            Optional[Dict]: –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –∏–ª–∏ None
        """
        # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        relevant = self.find_relevant_questions(query, threshold=0.5, top_k=1)
        
        if not relevant:
            logger.debug(f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è '{query}' –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return None
        
        # –ë–µ—Ä–µ–º —Å–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π
        best_match, similarity = relevant[0]
        
        # –ï—Å–ª–∏ —Å—Ö–æ–∂–µ—Å—Ç—å —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∞—è, –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º
        if similarity < 0.3:
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL, –µ—Å–ª–∏ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        url_valid = None
        url_status = None
        
        if check_urls and best_match.get('legal_url'):
            url_valid, url_status = await self.check_url_validity(best_match['legal_url'])
        
        return {
            'question': best_match['question'],
            'answer': best_match['short_answer'],
            'legal_reference': best_match['legal_reference'],
            'legal_url': best_match['legal_url'],
            'block': best_match['block'],
            'current_as_of': best_match['current_as_of'],
            'similarity_score': similarity,
            'url_valid': url_valid,
            'url_status': url_status
        }
    
    def format_answer_for_user(self, answer_data: Dict) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            answer_data: –î–∞–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç–∞ –∏–∑ get_answer_with_validation
            
        Returns:
            str: –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        text_parts = []
        
        # –ó–∞–≥–æ–ª–æ–≤–æ–∫
        text_parts.append(f"üìö <b>–ù–∞–π–¥–µ–Ω–æ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π</b>")
        text_parts.append(f"<b>–ö–∞—Ç–µ–≥–æ—Ä–∏—è:</b> {answer_data['block']}")
        text_parts.append("")
        
        # –í–æ–ø—Ä–æ—Å
        text_parts.append(f"<b>–í–æ–ø—Ä–æ—Å:</b> {answer_data['question']}")
        text_parts.append("")
        
        # –û—Ç–≤–µ—Ç
        text_parts.append(f"<b>–û—Ç–≤–µ—Ç:</b> {answer_data['answer']}")
        text_parts.append("")
        
        # –ü—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞ (—Ç–æ–ª—å–∫–æ –¥–ª—è –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)
        legal_ref = answer_data.get('legal_reference', '')
        legal_url = answer_data.get('legal_url', '')
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç–∞—Ç—å—é –¢–ö –†–§
        if legal_ref and '–¢–ö –†–§' in legal_ref:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ –∏–∑ –ø—Ä–∞–≤–æ–≤–æ–π –±–∞–∑—ã
            article_match = self._extract_article_number(legal_ref)
            if article_match:
                correct_url = f"https://www.consultant.ru/document/cons_doc_LAW_34683/{article_match}/"
                text_parts.append(f"<b>–ü—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞:</b> {legal_ref}")
                text_parts.append(f"‚úÖ <b>–°—Å—ã–ª–∫–∞:</b> {correct_url}")
            else:
                text_parts.append(f"<b>–ü—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞:</b> {legal_ref}")
                text_parts.append(f"<i>‚ÑπÔ∏è –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º</i>")
        elif legal_url and self._is_government_source(legal_url):
            text_parts.append(f"<b>–ü—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞:</b> {legal_ref}")
            url_emoji = "‚úÖ" if answer_data.get('url_valid') else "‚ö†Ô∏è"
            text_parts.append(f"{url_emoji} <b>–°—Å—ã–ª–∫–∞:</b> {legal_url}")
            
            if answer_data.get('url_valid') is False:
                text_parts.append(f"<i>‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: —Å—Å—ã–ª–∫–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ (–∫–æ–¥ {answer_data.get('url_status', 'N/A')})</i>")
        elif legal_ref:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∞–≤–æ–≤—É—é –±–∞–∑—É –±–µ–∑ —Å—Å—ã–ª–∫–∏, –µ—Å–ª–∏ —Å—Å—ã–ª–∫–∞ –Ω–µ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è
            text_parts.append(f"<b>–ü—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞:</b> {legal_ref}")
            text_parts.append(f"<i>‚ÑπÔ∏è –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º</i>")
        else:
            text_parts.append(f"<i>‚ÑπÔ∏è –î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º</i>")
        
        # –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å
        text_parts.append("")
        text_parts.append(f"<i>–ê–∫—Ç—É–∞–ª—å–Ω–æ –Ω–∞: {answer_data['current_as_of']}</i>")
        text_parts.append(f"<i>–°—Ç–µ–ø–µ–Ω—å —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è: {answer_data['similarity_score']:.0%}</i>")
        
        return "\n".join(text_parts)
    
    def _is_government_source(self, url: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º
        
        Args:
            url: URL –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            
        Returns:
            bool: True –µ—Å–ª–∏ —ç—Ç–æ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫
        """
        if not url:
            return False
        
        # –°–ø–∏—Å–æ–∫ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–æ–º–µ–Ω–æ–≤
        government_domains = [
            'kremlin.ru',           # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –ü—Ä–µ–∑–∏–¥–µ–Ω—Ç–∞
            'government.ru',        # –ü—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –†–§
            'duma.gov.ru',          # –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–∞—è –î—É–º–∞
            'council.gov.ru',       # –°–æ–≤–µ—Ç –§–µ–¥–µ—Ä–∞—Ü–∏–∏
            'minjust.gov.ru',       # –ú–∏–Ω—é—Å—Ç
            'mintrud.gov.ru',       # –ú–∏–Ω—Ç—Ä—É–¥
            'rostrud.gov.ru',       # –†–æ—Å—Ç—Ä—É–¥
            'gks.ru',               # –†–æ—Å—Å—Ç–∞—Ç
            'consultant.ru',        # –ö–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç–ü–ª—é—Å (—Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ)
            'pravo.gov.ru',         # –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–ø–æ—Ä—Ç–∞–ª –ø—Ä–∞–≤–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            'fzrf.sudrf.ru',        # –§–µ–¥–µ—Ä–∞–ª—å–Ω—ã–µ –∑–∞–∫–æ–Ω—ã –†–§
            'docs.cntd.ru',         # –¢–µ—Ö—ç–∫—Å–ø–µ—Ä—Ç
            'rulaws.ru',            # –†–£–õ–ê–í–°
            'zakonrf.info',         # –ó–∞–∫–æ–Ω –†–§
            'fstec.ru',             # –§–°–¢–≠–ö
            'fsb.ru',               # –§–°–ë
            'mvd.ru',               # –ú–í–î
            'rosgvard.ru',          # –†–æ—Å–≥–≤–∞—Ä–¥–∏—è
            'gost.ru',              # –†–æ—Å—Å—Ç–∞–Ω–¥–∞—Ä—Ç
            'rospotrebnadzor.ru',   # –†–æ—Å–ø–æ—Ç—Ä–µ–±–Ω–∞–¥–∑–æ—Ä
            'roszdravnadzor.gov.ru', # –†–æ—Å–∑–¥—Ä–∞–≤–Ω–∞–¥–∑–æ—Ä
            'gost.ru',              # –†–æ—Å—Å—Ç–∞–Ω–¥–∞—Ä—Ç
            'minzdrav.gov.ru',      # –ú–∏–Ω–∑–¥—Ä–∞–≤
            'edu.gov.ru',           # –ú–∏–Ω–æ–±—Ä
            'minobrnauki.gov.ru',   # –ú–∏–Ω–æ–±—Ä–Ω–∞—É–∫–∏
        ]
        
        url_lower = url.lower()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ URL –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã–π –¥–æ–º–µ–Ω
        for domain in government_domains:
            if domain in url_lower:
                return True
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –¥–æ–º–µ–Ω—ã .gov.ru
        if '.gov.ru' in url_lower:
            return True
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ –ø–æ—Ä—Ç–∞–ª—ã –ø—Ä–∞–≤–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        if any(keyword in url_lower for keyword in ['pravo.gov.ru', 'fzrf.sudrf.ru']):
            return True
        
        return False
    
    def _extract_article_number(self, legal_ref: str) -> str:
        """
        –ò–∑–≤–ª–µ—á—å –Ω–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ –∏–∑ –ø—Ä–∞–≤–æ–≤–æ–π –±–∞–∑—ã
        
        Args:
            legal_ref: –ü—Ä–∞–≤–æ–≤–∞—è –±–∞–∑–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "–¢–ö –†–§ —Å—Ç. 209")
            
        Returns:
            str: –ù–æ–º–µ—Ä —Å—Ç–∞—Ç—å–∏ –¥–ª—è URL –∏–ª–∏ –ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
        """
        import re
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω "—Å—Ç. 209" –∏–ª–∏ "—Å—Ç.209"
        article_pattern = r'—Å—Ç\.\s*(\d+(?:\.\d+)?)'
        match = re.search(article_pattern, legal_ref)
        
        if match:
            article_num = match.group(1)
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π URL –¥–ª—è —Å—Ç–∞—Ç—å–∏ –¢–ö –†–§
            return f"st-{article_num}"
        
        return ""
    
    def get_statistics(self) -> Dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
        
        Returns:
            Dict: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        """
        if not self.faq_data:
            return {
                'total_questions': 0,
                'blocks': [],
                'questions_with_urls': 0,
                'questions_without_urls': 0
            }
        
        blocks = {}
        urls_count = 0
        
        for item in self.faq_data:
            block = item['block']
            blocks[block] = blocks.get(block, 0) + 1
            
            if item.get('legal_url') and item['legal_url'].strip():
                urls_count += 1
        
        return {
            'total_questions': len(self.faq_data),
            'blocks': blocks,
            'questions_with_urls': urls_count,
            'questions_without_urls': len(self.faq_data) - urls_count
        }


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
_knowledge_base: Optional[KnowledgeBase] = None


def get_knowledge_base() -> KnowledgeBase:
    """
    –ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (singleton)
    
    Returns:
        KnowledgeBase: –≠–∫–∑–µ–º–ø–ª—è—Ä –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
    """
    global _knowledge_base
    
    if _knowledge_base is None:
        _knowledge_base = KnowledgeBase()
    
    return _knowledge_base

